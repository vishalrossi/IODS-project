---
title: "chapter4.Rmd"
author: "Vishal"
date: "2/14/2017"
output: html_document
---

# Clustering and Classification

##### Access all the required libraries for the exercise
library(dplyr)
library(ggplot2)
library(tidyr)
library(corrplot)
library(reshape2)
library(plotly)


#### 2. Load and explore the data
 
```{r}
library(MASS)
```
```{r}
data("Boston")
dim(Boston)
str(Boston)
```

*This data tells about the housing values in sururbs of Boston.*

*It has 506 entries and 14 variables.*

#### 3. Summary of variables and Graphical overview of the data.

##### Summary of variables in the data
```{r}
summary(Boston)
```

##### Graphical overview
```{r}
cor_matrix<-cor(Boston) 
library(corrplot)
corrplot(cor_matrix, method="circle", type = "upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.6)
```

*From the correaltion matrix the findings are:*
*1. rad is highly positively correlated with tax.*
*2. dis is negatively correlated with index, nox and age.*
*3. lstat is negatively correlated with medv*


#####Explanation of 14 variables
crim = per capita crime rate by town
zn = proportion of residential land zoned for lots over 25,000 sq.ft
indus = proportion of non-retail business acres per town
chas = Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
nox = nitrogen oxides concentration (parts per 10 million)
rm = average number of rooms per dwelling
age = proportion of owner-occupied units built prior to 1940
dis = weighted mean of distances to five Boston employment centres
rad = index of accessibility to radial highways
tax = full-value property-tax rate per $10,000
ptratio = pupil-teacher ratio by town
black = 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
lstat = lower status of the population (percent)
medv = median value of owner-occupied homes in $1000s

##### Distribution of 14 variables
```{r}
hist(Boston$crim, col = "grey", main = "Distribution of crim")
hist(Boston$zn, col = "grey", main = "Distribution of zn")
hist(Boston$indus, col = "grey", main = "Distribution of indus")
hist(Boston$chas, col = "grey", main = "Distribution of chas")
hist(Boston$nox, col = "grey", main = "Distribution of nox")
hist(Boston$rm, col = "grey", main = "Distribution of rm")
hist(Boston$age, col = "grey", main = "Distribution of age")
hist(Boston$dis, col = "grey", main = "Distribution of dis")
hist(Boston$rad, col = "grey", main = "Distribution of rad")
hist(Boston$tax, col = "grey", main = "Distribution of tax")
hist(Boston$ptratio, col = "grey", main = "Distribution of ptratio")
hist(Boston$black, col = "grey", main = "Distribution of black")
hist(Boston$lstat, col = "grey", main = "Distribution of lstat")
hist(Boston$medv, col = "grey", main = "Distribution of medv")
```

*The variables that follow normal distribution are rm and medv. It can be also seen from summary for these variables as mean and median are closer.*

#### 4. Standardize the data

```{r}
##### Scale the variables
boston_scaled <- scale(Boston)


##### Summaries of the scaled variables
summary(boston_scaled)

##### Crime rate from scaled Boston dataset
#### change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)
scaled_crim <- boston_scaled$crim
summary(scaled_crim)

##### create a quantile vector of crim
bins <- quantile(scaled_crim)
bins

##### Create a categorical variable 'crime'
crime <- cut(scaled_crim, breaks = bins, include.lowest = TRUE, label = c("low", "med_low", "med_high", "high"))

##### Remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

##### Add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

##### Creatde test and training data
n <- nrow(boston_scaled)
ind <- sample(n,  size = n * 0.8)
train <- boston_scaled[ind,]
test <- boston_scaled[-ind,]
```

#### 5. LDA

```{r}
##### linear discriminant analysis
##### Using the categorical crime rate as the target variable and all the other variables in the dataset as predictor variables.
lda.fit <- lda(crime ~ ., data = train)
lda.fit

##### biplot
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}
classes <- as.numeric(train$crime)
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)
```


#### 6. Fitting LDA

```{r}
##### save the correct classes from test data
correct_classes <- test$crime

##### remove the crime variable from test data
test <- dplyr::select(test, -crime)
classes <- as.numeric(test$crime)

##### predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

##### cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
```

#####Result: Model is fairly accurate with its prediction. It did not do any mistakes with high crime area predicitions. It is more confused with low and med_low categories.

##### 7. Cluster
```{r}
data("Boston")
boston_scaled <- scale(Boston)
# euclidean distance matrix
dist_eu <- dist(boston_scaled)

# look at the summary of the distances
summary(dist_eu)

# manhattan distance matrix
dist_man <- dist(boston_scaled, method = "manhattan")

# look at the summary of the distances
summary(dist_man)

# k-means clustering
km <-kmeans(dist_eu, centers = 15)

# plot the Boston dataset with clusters
pairs(Boston, col = km$cluster)

# determine the number of clusters
set.seed(123)
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(dist_eu, k)$tot.withinss})

# visualize the results
plot(1:k_max, twcss, type='b')

# k-means clustering
#### After calculating total within sum of squares and plotting it, sharpest drop is between 1 and 2, so 2 is probably the optimal cluster amount.
km <-kmeans(dist_eu, centers = 2)

# plot the Boston dataset with clusters
pairs(Boston, col = km$cluster)
```

##### It seems that most of the times the black and red data points separate from each other and are rarely mixed. In some plots a clear pattern can be seen, such as plot between tax and crim. 
